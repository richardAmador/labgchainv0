C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0> & C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Scripts\python.exe c:/Users/richar.amadorl/Documents/AI_Engineer/LangChain/labgchainv0/langchain_v0.py
Traceback (most recent call last):
  File "c:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\langchain_v0.py", line 16, in <module>
    print(chat.invoke("Say hello in one short sentence."))
          ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 395, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1023, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 840, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1089, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\langchain_openai\chat_models\base.py", line 1184, in _generate
    raise e
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\langchain_openai\chat_models\base.py", line 1179, in _generate
    raw_response = self.client.with_raw_response.create(**payload)
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\openai\_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1147, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<46 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\richar.amadorl\Documents\AI_Engineer\LangChain\labgchainv0\.venv\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'status': 'failure', 'message': 'Either x-portkey-config or x-portkey-provider header is required'}